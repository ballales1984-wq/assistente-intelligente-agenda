"""
Groq LLM Assistant - Velocissimo e gratuito!
"""

import os
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)

# Import condizionale Groq
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    logger.warning("Groq non installato. Usa: pip install groq")


class GroqAssistant:
    """Assistente AI basato su Groq (velocissimo!)"""
    
    def __init__(self):
        self.client = None
        self.api_key = os.getenv('GROQ_API_KEY')
        
        if GROQ_AVAILABLE and self.api_key:
            try:
                self.client = Groq(api_key=self.api_key)
                logger.info("âœ… Groq client inizializzato")
            except Exception as e:
                logger.error(f"âŒ Errore inizializzazione Groq: {e}")
        else:
            if not GROQ_AVAILABLE:
                logger.warning("âš ï¸ Groq non disponibile - installa: pip install groq")
            if not self.api_key:
                logger.warning("âš ï¸ GROQ_API_KEY non configurata - imposta variabile ambiente")
    
    def is_available(self) -> bool:
        """Verifica se Groq Ã¨ disponibile"""
        return self.client is not None
    
    def chat(
        self,
        messaggio: str,
        system_prompt: Optional[str] = None,
        model: str = "llama-3.1-70b-versatile",
        max_tokens: int = 1024,
        temperature: float = 0.7,
        lang: str = 'it'
    ) -> Dict[str, Any]:
        """
        Chat con Groq LLM
        
        Modelli disponibili:
        - llama-3.1-70b-versatile (migliore, bilanciato)
        - llama-3.1-8b-instant (velocissimo)
        - mixtral-8x7b-32768 (context lungo)
        
        Args:
            messaggio: Messaggio utente
            system_prompt: Prompt di sistema (opzionale)
            model: Modello da usare
            max_tokens: Massimo token risposta
            temperature: CreativitÃ  (0-1)
            lang: Lingua ('it', 'en', 'es', etc.)
        
        Returns:
            Dict con risposta e metadata
        """
        if not self.is_available():
            return {
                "success": False,
                "error": "Groq non disponibile",
                "risposta": "âš ï¸ Chat AI non disponibile. Usa i comandi NLP standard!"
            }
        
        try:
            # System prompt personalizzato per lingua
            if not system_prompt:
                prompts = {
                    'it': "Sei un assistente personale intelligente. Aiuti l'utente a organizzare obiettivi, impegni, spese e diario. Rispondi in modo conciso e amichevole in italiano.",
                    'en': "You are a smart personal assistant. You help users organize goals, commitments, expenses, and diary. Reply concisely and friendly in English.",
                    'es': "Eres un asistente personal inteligente. Ayudas a los usuarios a organizar objetivos, compromisos, gastos y diario. Responde de forma concisa y amigable en espaÃ±ol.",
                    'zh': "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä¸ªäººåŠ©æ‰‹ã€‚ä½ å¸®åŠ©ç”¨æˆ·ç»„ç»‡ç›®æ ‡ã€æ‰¿è¯ºã€æ”¯å‡ºå’Œæ—¥è®°ã€‚ç”¨ä¸­æ–‡ç®€æ´å‹å¥½åœ°å›žå¤ã€‚",
                    'ru': "Ð’Ñ‹ ÑƒÐ¼Ð½Ñ‹Ð¹ Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº. Ð’Ñ‹ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ†ÐµÐ»Ð¸, Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°, Ñ€Ð°ÑÑ…Ð¾Ð´Ñ‹ Ð¸ Ð´Ð½ÐµÐ²Ð½Ð¸Ðº. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ð¾ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.",
                    'ar': "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø´Ø®ØµÙŠ Ø°ÙƒÙŠ. ØªØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù„Ù‰ ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª ÙˆØ§Ù„Ù†ÙÙ‚Ø§Øª ÙˆØ§Ù„ÙŠÙˆÙ…ÙŠØ§Øª. Ø£Ø¬Ø¨ Ø¨Ø¥ÙŠØ¬Ø§Ø² ÙˆÙˆØ¯ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.",
                }
                system_prompt = prompts.get(lang, prompts['it'])
            
            # Chiamata Groq
            logger.info(f"ðŸš€ Groq chat: model={model}, lang={lang}")
            
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": messaggio}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
            )
            
            risposta = response.choices[0].message.content
            
            # Metadata
            metadata = {
                "model": model,
                "tokens_used": response.usage.total_tokens,
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "finish_reason": response.choices[0].finish_reason,
            }
            
            logger.info(f"âœ… Groq risposta: {metadata['tokens_used']} tokens")
            
            return {
                "success": True,
                "risposta": risposta,
                "metadata": metadata,
                "provider": "groq"
            }
            
        except Exception as e:
            logger.error(f"âŒ Errore Groq chat: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "risposta": "âš ï¸ Errore temporaneo. Riprova!"
            }
    
    def analizza_comando(
        self,
        messaggio: str,
        contesto: Optional[Dict] = None,
        lang: str = 'it'
    ) -> Dict[str, Any]:
        """
        Analizza comando complesso con AI
        Usato quando NLP regex fallisce
        
        Args:
            messaggio: Comando utente
            contesto: Contesto opzionale (obiettivi, impegni, etc.)
            lang: Lingua
        
        Returns:
            Dict con tipo, dati estratti, suggerimento
        """
        system_prompts = {
            'it': """Sei un parser intelligente di comandi per agenda.
Estrai da messaggi naturali: obiettivi, impegni, spese, riflessioni diario.

Rispondi SOLO JSON:
{
  "tipo": "obiettivo|impegno|spesa|diario|domanda",
  "dati": {...},
  "suggerimento": "..."
}""",
            'en': """You are a smart command parser for agenda.
Extract from natural messages: goals, commitments, expenses, diary reflections.

Reply ONLY JSON:
{
  "type": "goal|commitment|expense|diary|question",
  "data": {...},
  "suggestion": "..."
}""",
        }
        
        prompt = system_prompts.get(lang, system_prompts['it'])
        
        return self.chat(
            messaggio=messaggio,
            system_prompt=prompt,
            model="llama-3.1-8b-instant",  # Veloce per parsing
            max_tokens=256,
            temperature=0.3,  # Bassa temperatura per parsing preciso
            lang=lang
        )


# Istanza globale
groq_assistant = GroqAssistant()

